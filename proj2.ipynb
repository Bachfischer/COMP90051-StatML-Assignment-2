{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit. These are the only imports permitted.\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.tree import DecisionTreeClassifier   # for Task 4\n",
    "from sklearn.base import clone                    # optional for Task 4\n",
    "import matplotlib.pyplot as plt                   # for Task 5\n",
    "from sklearn.metrics.pairwise import rbf_kernel   # for Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB(ABC):\n",
    "    \"\"\"Base class for a contextual multi-armed bandit (MAB)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms):\n",
    "        if not np.issubdtype(type(n_arms), np.integer):\n",
    "            raise TypeError(\"`n_arms` must be an integer\")\n",
    "        if not n_arms >= 0:\n",
    "            raise ValueError(\"`n_arms` must be non-negative\")\n",
    "        self.n_arms = n_arms\n",
    "        # your code here (if you like)\n",
    "        \n",
    "    @abstractmethod\n",
    "    def play(self, context):\n",
    "        \"\"\"Play a round\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        context : float numpy.ndarray, shape (n_arms, n_dims), optional\n",
    "            An array of context vectors presented to the MAB. The 0-th \n",
    "            axis indexes the arms, and the 1-st axis indexes the features.\n",
    "            Non-contextual bandits accept a context of None.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        arm : int\n",
    "            Integer index of the arm played this round. Should be in the set \n",
    "            {0, ..., n_arms - 1}.\n",
    "        \"\"\"\n",
    "        # your code here (if you like)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self, arm, reward, context):\n",
    "        \"\"\"Update the internal state of the MAB after a play\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arm : int\n",
    "            Integer index of the played arm in the set {0, ..., n_arms - 1}.\n",
    "        \n",
    "        reward : float\n",
    "            Reward received from the arm.\n",
    "        \n",
    "        context : float numpy.ndarray, shape (n_arms, n_dims), optional\n",
    "            An array of context vectors that was presented to the MAB. The \n",
    "            0-th axis indexes the arms, and the 1-st axis indexes the \n",
    "            features. Non-contextual bandits accept a context of None. \n",
    "        \"\"\"\n",
    "        # your code here (if you like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global functions here, if required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement Îµ-greedy and UCB MABs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsGreedy(MAB):\n",
    "    \"\"\"Epsilon-Greedy multi-armed bandit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms\n",
    "\n",
    "    epsilon : float\n",
    "        Explore probability. Must be in the interval [0, 1].\n",
    "\n",
    "    Q0 : float, default=np.inf\n",
    "        Initial value for the arms.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, epsilon, Q0=np.inf):\n",
    "        super().__init__(n_arms)\n",
    "        # your code here\n",
    "    \n",
    "    def play(self, context=None):\n",
    "        super().play(context)\n",
    "        # your code here\n",
    "        \n",
    "    def update(self, arm, reward, context=None):\n",
    "        super().update(arm, reward, context)\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(MAB):\n",
    "    \"\"\"Upper Confidence Bound (UCB) multi-armed bandit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    rho : float\n",
    "        Positive real explore-exploit parameter.\n",
    "\n",
    "    Q0 : float, default=np.inf\n",
    "        Initial value for the arms.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, rho, Q0=np.inf):\n",
    "        super().__init__(n_arms)\n",
    "        # your code here\n",
    "    \n",
    "    def play(self, context=None):\n",
    "        super().play(context)\n",
    "        # your code here\n",
    "        \n",
    "    def update(self, arm, reward, context=None):\n",
    "        super().update(arm, reward, context)\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement off-policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offlineEvaluate(mab, arms, rewards, contexts, n_rounds=None):\n",
    "    \"\"\"Offline evaluation of a multi-armed bandit\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mab : instance of MAB\n",
    "        MAB to evaluate.\n",
    "    \n",
    "    arms : integer numpy.ndarray, shape (n_events,) \n",
    "        Array containing the history of pulled arms, represented as integer \n",
    "        indices in the set {0, ..., mab.n_arms}\n",
    "    \n",
    "    rewards : float numpy.ndarray, shape (n_events,)\n",
    "        Array containing the history of rewards.\n",
    "    \n",
    "    contexts : float numpy.ndarray, shape (n_events, n_arms, n_dims)\n",
    "        Array containing the history of contexts presented to the arms. \n",
    "        The 0-th axis indexes the events in the history, the 1-st axis \n",
    "        indexes the arms and the 2-nd axis indexed the features.\n",
    "        \n",
    "    n_rounds : int, default=None\n",
    "        Number of matching events to evaluate the MAB on. If None, \n",
    "        continue evaluating until the historical events are exhausted.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : float numpy.ndarray\n",
    "        Rewards for the matching events.\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = EpsGreedy(10, 0.05)\n",
    "results_EpsGreedy = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('EpsGreedy average reward', np.mean(results_EpsGreedy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = UCB(10, 1.0)\n",
    "results_UCB = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('UCB average reward', np.mean(results_UCB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement LinUCB contextual MAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB(MAB):\n",
    "    \"\"\"Contextual multi-armed bandit (LinUCB)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    n_dims : int\n",
    "        Number of features for each arm's context.\n",
    "\n",
    "    alpha : float\n",
    "        Positive real explore-exploit parameter.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, n_dims, alpha):\n",
    "        super().__init__(n_arms)\n",
    "        # your code here\n",
    "    \n",
    "    def play(self, context):\n",
    "        super().play(context)\n",
    "        # your code here\n",
    "    \n",
    "    def update(self, arm, reward, context):\n",
    "        super().update(arm, reward, context)\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = LinUCB(10, 10, 1.0)\n",
    "results_LinUCB = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('LinUCB average reward', np.mean(results_LinUCB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement TreeBootstrap contextual MAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeBootstrap(MAB):\n",
    "    \"\"\"Contextual Thompson sampled multi-armed bandit (TreeBootstrap)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    n_dims : int\n",
    "        Number of features for each arm's context.\n",
    "\n",
    "    tree : instance of sklearn.tree.DecisionTreeClassifier, optional\n",
    "        Decision tree to use for predicting the expected future reward. \n",
    "        Defaults to sklearn.tree.DecisionTreeClassifier().\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, n_dims, tree=DecisionTreeClassifier()):\n",
    "        super().__init__(n_arms)\n",
    "        # your code here\n",
    "        \n",
    "    def play(self, context):\n",
    "        super().play(context)\n",
    "        # your code here\n",
    "    \n",
    "    def update(self, arm, reward, context):\n",
    "        super().update(arm, reward, context)\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = TreeBootstrap(10, 10)\n",
    "results_TreeBootstrap = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('TreeBootstrap average reward', np.mean(results_TreeBootstrap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and hyperparameter tuning for LinUCB\n",
    "### 5.A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement KernelUCB contextual MAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelUCB(MAB):\n",
    "    \"\"\"Kernelised contextual multi-armed bandit (Kernelised LinUCB)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    n_dims : int\n",
    "        Number of features for each arm's context.\n",
    "\n",
    "    gamma : float\n",
    "        Positive real explore-exploit parameter.\n",
    "    \n",
    "    eta : float\n",
    "        Positive real explore-exploit parameter.\n",
    "    \n",
    "    kern : callable\n",
    "        A kernel function from sklearn.metrics.pairwise.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, n_dims, gamma, eta, kern):\n",
    "        super().__init__(n_arms)\n",
    "        # your code here\n",
    "        \n",
    "    def play(self, context):\n",
    "        super().play(context)\n",
    "        # your code here\n",
    "    \n",
    "    def update(self, arm, reward, context):\n",
    "        super().update(arm, reward, context)\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = KernelUCB(10, 10, 1.0, 0.1, rbf_kernel)\n",
    "results_KernelUCB = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('KernelUCB average reward', np.mean(results_KernelUCB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your plotting code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
